<!DOCTYPE html>
<html lang="cn">
<head>
        <meta charset="utf-8" />
        <title>耕有田，读有书 - supervised learning</title>
        <link rel="stylesheet" href="http://forin-xyz.github.io/output/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="http://github.com/forin-xyz/">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="http://forin-xyz.github.io/output/">耕有田，读有书 </a></h1>
                <nav><ul>
                    <li><a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a></li>
                    <li><a href="http://forin-xyz.github.io/output/category/python.html">Python</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://forin-xyz.github.io/output/2016/03/scikit-learnjian-yao.html">scikit-learn简要</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-03-03T23:00:00+08:00">
                发表于: 2016年03月03日 星期Thu
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/python.html">Python</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/python.html">python</a> <a href="http://forin-xyz.github.io/output/tag/scikit-learn.html">scikit-learn</a> <a href="http://forin-xyz.github.io/output/tag/machine-learning.html">machine learning</a> <a href="http://forin-xyz.github.io/output/tag/supervised-learning.html">supervised learning</a> <a href="http://forin-xyz.github.io/output/tag/non-supervised-learning.html">non-supervised learning</a> <a href="http://forin-xyz.github.io/output/tag/classfication.html">classfication</a> <a href="http://forin-xyz.github.io/output/tag/prediction.html">prediction</a> <a href="http://forin-xyz.github.io/output/tag/clustering.html">clustering</a> <a href="http://forin-xyz.github.io/output/tag/dimensionality.html">dimensionality</a> </p>
</footer><!-- /.post-info --><h2>分类 Classfication</h2>
<h3>线性模型 Generalized Linear Models</h3>
<h4>线性回归 -&gt; 普通最小二乘法 Linear Regression -&gt; Oridinary Least Squares</h4>
<div class="math">$$
min\limits_w || Xw - y ||_2^2
$$</div>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># clf -&gt; classfication</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># fit(X, y) -&gt; 学习模型</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span> <span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># 非常数项系数</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">])</span>
</pre></div>


<h4>脊回归 Ridge Regression -&gt; 带惩罚函数的最小二乘法 Least Squares by imposing a penalty on the size of coefficients.</h4>
<div class="math">$$
min\limits_w {|| Xw - y ||_2^2 + \alpha ||w||_2^2}, \alpha \geq 0
$$</div>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span> <span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 
<span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
      <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.34545455</span><span class="p">,</span>  <span class="mf">0.34545455</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># 常数项系数 or 截距</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span> 
<span class="mf">0.13636</span><span class="o">.</span>
</pre></div>


<h5>获取正则项或惩罚函数系数 -&gt; 交叉验证 Cross Validation</h5>
<p>..</p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># 多了一个后缀CV</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>       
<span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">alpha_</span>                                      
<span class="mf">0.1</span>
</pre></div>


<h4>Lasson 拉索</h4>
<p>估计稀疏系数时会有用, 他更倾向于使用较少的参数值, 如果问题需要减少变量个数, 它将是非常有用的.</p>
<div class="math">$$
min\limits_w {\frac{1}{2n_samples}{|| Xw - y ||_2^2 + \alpha \lVert w \rVert_1}}, \alpha \geq 0
$$</div>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
   <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
   <span class="n">selection</span><span class="o">=</span><span class="s1">&#39;cyclic&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># 预测</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.8</span><span class="p">])</span>
</pre></div>


<h5>确定正则项系数</h5>
<h6>使用交叉验证</h6>
<p><strong>LassoCV</strong></p>
<p><strong>LassoLarsCV</strong></p>
<h6>基于信息标准的模型选择 Information-criteria based model selection</h6>
<p><strong>LassoLarsIC</strong></p>
<h4>Elastic Net</h4>
<div class="math">$$
min\limits_w {\frac{1}{2n_samples}{|| Xw - y ||_2^2 + \alpha \rho \lVert w \rVert _1 + \alpha (1 -  \rho) \lVert w \rVert _2^2 }}, \alpha \geq 0, \rho \geq 0
$$</div>
<h4>Multi-task Lasso</h4>
<h4>Least Angle Regression</h4>
<h4>LARS Lasso</h4>
<h3>线性和二次判别分析 Linear and Quadratic Discriminant Analysis</h3>
<h3>Kernle Ridge Regrssion 带核脊回归</h3>
<h3>支持向量机 Support Machines</h3>
<h3>随机梯度下降法 Stochastic Gredient Descent</h3>
<h3>k近邻 Nearest Neighbors</h3>
<h3>Gaussian Process 高斯过程</h3>
<h3>Cross decomposion</h3>
<h3>朴素贝叶斯 Naive Bayes</h3>
<h3>决策树 Decision Trees</h3>
<h3>Ensemble Methods</h3>
<h3>Multiclass and multilabel algorithms 多类多标签算法</h3>
<h3>特征选择 feature selection</h3>
<h3>Semi-Supervised</h3>
<h3>Isotonic Regression</h3>
<h3>Probability calibration</h3>
<h2>Clustering 聚类</h2>
<h2>降维 Dimensionality reduction</h2>
<h3>主成分分析 Principal component analysis (PCA)</h3>
<h4>Exact PCA and probabilistic interpretation</h4>
<p>Linear dimensionality reduction using Singular Value Decomposition of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.</p>
<p>算法(PCA):</p>
<blockquote>
<p>输入:<span class="math">\(X \in K^{N*n}\)</span>, 需要降到的维数m &lt; n</p>
<p>输出: 转换矩阵$M \in K^{n*m}</p>
<p>(1) 求出X的每一列均值, 并用X减去, 得到矩阵<span class="math">\(Y\)</span>是的Y每一列的均值为0</p>
<p>(2) 求Y列与列之间的协方差矩阵<span class="math">\(C\)</span>,
<div class="math">$$C[i, j] = Y[:,i] \cdot Y[:, j] / N$$</div>
等价于
$$C = X^T X</p>
<p>(3) 求出协方差矩阵C的特征值和特征向量</p>
<p>(4) 取特征值最大的m个特征值及其对应的特征向量</p>
<p>(5) 取出的特征向量组合成n*m矩阵即为所求<span class="math">\(M=(\alpha _1, \alpha _2, \cdots, \alpha _m) \in K^{n*m}\)</span></p>
</blockquote>
<h4>Incremental PCA</h4>
<p>Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.</p>
<h4>Approximate PCA</h4>
<h4>Kernel PCA</h4>
<h4>Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)</h4>
<h3>截断奇异值分解和潜在语义分析 Truncated singular value decomposition and latent semantic analysis</h3>
<h3>Dictionary Learning</h3>
<h3>factor analysis</h3>
<h3>Independent component analysis (ICA) 独立成分分析</h3>
<h3>Non-negative matrix factorization (NMF or NNMF) 非负矩阵分解</h3>
<h3>Latent Dirichlet Allocation (LDA)</h3>
<h2>模型选择 Model selection and evaluation</h2>
<h2>Preprocessing data 预处理数据</h2>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://forin-xyz.github.io/tutorial.html">我的导航</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://weibo.com/u/1756830393">微博</a></li>
                            <li><a href="http://github.com/forin-xyz">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
        </footer><!-- /#contentinfo -->

</body>
</html>