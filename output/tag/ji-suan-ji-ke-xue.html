<!DOCTYPE html>
<html lang="cn">
<head>
        <meta charset="utf-8" />
        <title>耕有田，读有书 - 计算机科学</title>
        <link rel="stylesheet" href="http://forin-xyz.github.io/output/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="http://github.com/forin-xyz/">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="http://forin-xyz.github.io/output/">耕有田，读有书 </a></h1>
                <nav><ul>
                    <li><a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a></li>
                    <li><a href="http://forin-xyz.github.io/output/category/python.html">Python</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-2-cong-gan-zhi-ji-dao-zhi-chi-xiang-liang-ji.html">《统计学习方法》读书笔记2-从感知机到支持向量机，含python源码</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-02-18T23:00:00+08:00">
                发表于: 2016年02月18日 星期四
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/ji-qi-xue-xi.html">机器学习</a> <a href="http://forin-xyz.github.io/output/tag/gan-zhi-ji.html">感知机</a> <a href="http://forin-xyz.github.io/output/tag/perceptron.html">perceptron</a> <a href="http://forin-xyz.github.io/output/tag/zhi-chi-xiang-liang-ji.html">支持向量机</a> <a href="http://forin-xyz.github.io/output/tag/super-vector-machines.html">super vector machines</a> <a href="http://forin-xyz.github.io/output/tag/svm.html">SVM</a> <a href="http://forin-xyz.github.io/output/tag/python.html">python</a> <a href="http://forin-xyz.github.io/output/tag/ji-suan-ji-ke-xue.html">计算机科学</a> <a href="http://forin-xyz.github.io/output/tag/shu-xue-yuan-li.html">数学原理</a> <a href="http://forin-xyz.github.io/output/tag/du-shu-bi-ji.html">读书笔记</a> </p>
</footer><!-- /.post-info --><h2>感知机</h2>
<h3>感知机的定义</h3>
<p>从输入空间\( \mathit{X} \subseteq R^n\)到输出空间\(\mathit{Y} \subseteq {+1, -1}\)的函数
</p>
<div class="math">$$f(x)=sign(w\cdot\mathit{x} + b)$$</div>
<p>被称为感知机。输入x表示实例的特征向量，输出y表示实例的类别，w和b为感知机模型的参数，分别叫做权值向量和偏置。\(sign(x)\)是符号函数，即
</p>
<div class="math">$$sign(x)=
\begin{cases}
+1,&amp;  &amp;{x \geq 0}\\
-1,&amp;  &amp;{ x &lt; 0 }
\end{cases}$$</div>
<p>。</p>
<h3>几何解释</h3>
<p>线性方程</p>
<div class="math">$$w\cdot\mathit{x} + b = 0$$</div>
<p>对应于特征空间\(R^n\)中的一个超平面，其中\(w\)是超平面的法向量，\(b\)是超平面的截距。这个超平面恰好将特征空间的点分为两个部分，分别对应于正、负两类实例。</p>
<h3>数据集的线性可分性</h3>
<p>给定一个数据集</p>
<div class="math">$$T = \{(x_{1},y_{1}),(x_{2},y_{2}),\cdots, (x_{N},y_{N})\},$$</div>
<p>其中, \(x_{i}\)属于特征空间的点，如果存在某个超平面\(S\)</p>
<div class="math">$$w \cdot \mathit{x} + b = 0$$</div>
<p>能够完全将数据集的实例点正确分类即对所有的\(i\)都有$$\mathit{y}<em i="i">{i}(w \cdot \mathit{x</em>} + b) &gt;0, 则称线性数据集\(T\)为线性可分数据集(linearly separable data set); 否则, 称数据集\(T\)线性可分。</p>
<h3>感知机的损失函数</h3>
<p>对于正确分类的点，其损失函数应该为0, 对于错误分类的点, 其损失函数应该大于0，整个的经验损失函数就是训练集中所有实例点的损失函数的和。</p>
<p>如果训练集是可分线性数据集，感知机模型就是寻找到一个能够将正实例点和负实例点完全分开的分离超平面，亦即是的整个经验损失函数为0。</p>
<p>考虑到计算的方便，我们可以这样定义经验损失函数
</p>
<div class="math">$$L(w, b)= \sum_{i=1}^{N}{\lbrack y_{i}(w \cdot x_{i} + b)\rbrack_{+}}$$</div>
<p>
其中\(\lbrack x\rbrack_{+}\)表示取不小于\(x\)的最小的非负值，即
</p>
<div class="math">$$\lbrack x\rbrack_{+} = \begin{cases}
x, &amp;   &amp;x \gt 0\\
0, &amp;   &amp;0 \leq 0
\end{cases}$$</div>
<p>于是感知机模型就等价于最优化问题</p>
<div class="math">$$\min{L(w, b)},$$</div>
<p>因为经验损失函数大于等于0，也就是求出相应的不全为0的w, b使得\(L(w, b) = 0\).</p>
<h3>感知机学习算法的原始形式</h3>
<p>感知机学习算法是误分类驱动的，具体采用随机梯度下降法(stochastic gradient descent). 具体而言就是每一次对其中一个误分类点使用梯度下降法使其的经验损失函数值减小。</p>
<p>随机选取一个误分类点\((x_{i}, y_{i})\), 其损失函数对应的梯度为:
</p>
<div class="math">$$\begin(aligned)\delta_{w} L_{i}(w, b) &amp;= -y_{i}x_{i}\\
$$</div>
<p>\delta_{b} L_{i}(w, b) &amp;= -y_{i}
\end{aligned}$$</p>
<p>这样对应的迭代函数为:</p>
<div class="math">$$w \gets w + \eta y_{i}x_{i}$$</div>
<div class="math">$$b \gets b + \eta y_{i}$$</div>
<p>直到\(y_{i}(w\cdotx_{i} + b) \leq 0\)为止。</p>
<p>其中\(\eta (0 &lt; \eta \leq 1)\)是步长，在统计学习中又称为学习率(learning rate)。</p>
<h4>感知机学习算法的原始形式</h4>
<blockquote>
<p>输入：\(T={(x_{1},y_{1}),(x_{2},y_{2}), \cdots ,(x_{N},y_{N})}\)（其中\(x_{i} \in R^n\)，\(y_{i}\)∈Y={-1,+1}，i=1,2...N，学习速率为η）
输出：w, b;感知机模型f(x)=sign(w·x+b)
(1) 初始化w=0,b=0
(2) 在训练数据集中选取（xi, yi）
(3) 如果\(y_{i}(w \cdot x_{i} + b ) \leq 0\)
         <div class="math">$$w = w + {\eta}y_{i}x_{i}$$</div>
<div class="math">$$b = b + {\eta}y_{i}$$</div>
  (4) 转至（2）</p>
</blockquote>
<h3>感知机学习算法的原始形式的python源码</h3>
<p><a href="https://github.com/forin-xyz/statistical-learning-method/blob/master/perceptron_origin.py" title="gihub">感知机学习算法原始形式</a></p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">perceptron_origin_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">goon</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">goon</span><span class="p">:</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">while</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">break</span>
        <span class="n">goon</span> <span class="o">=</span> <span class="n">flag</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>


<span class="k">def</span> <span class="nf">test_perceptron_origin_train</span><span class="p">():</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">perceptron_origin_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="kp">all</span><span class="p">()</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">==</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;test perceptron_origin_train sucessful!&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;test perceptron_origin_train fail!&#39;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">test_perceptron_origin_train</span><span class="p">()</span>
</pre></div>


<h3>算法的收敛性</h3>
<p>证明对于线性可分数据集感知机学习算法原始形式收敛。</p>
<h4>Novikoff定理</h4>
<p>定理表明，误分类的次数\(k\)是有上界的，经过有限次搜索可以找到将训练数据完全分开的分离超平面，即感知机学习的原始形式算法收敛。</p>
<h3>感知机学习算法的对偶形式</h3>
<p>通过对原始形式的感知机学习算法的观察, 我们发现，如果假设初始值\(w_{0}, b_{0}\)为0的话， 最终学习得到的\(w, b\)分别是\(y_{i}x_{i}, y_{i}\)的线性组合，即
</p>
<div class="math">$$\begin{aligned}
w &amp;= \sum\limits_{i=1}^{N}\alpha_{i}y_{i}x_{i}\\
b &amp;= \sum\limits_{i=1}^{N}\alpha_{i}y_{i}
\end{aligned}$$</div>
<p>
这里，\(\appha_{i} \geq 0\), 当 \(\eta = 1\)时，表示第i个实例点由于误分而进行更新的次数</p>
<h4>感知机学习算法的对偶形式</h4>
<blockquote>
<p>输入：T={(x1,y1),(x2,y2)...(xN,yN)}（其中xi∈X=Rn，yi∈Y={-1, +1}，i=1,2...N，学习速率为η）
   输出：a,b;感知机模型f(x)=sign(w·x+b)
   (1) 初始化\(\alpha=0,b=0\)
   (2) 在训练数据集中选取（xi, yi）
   (3) 如果 \(y_{i}(\sum\limits_{j=1}^{N}{\alpha_{j}y{j}x_{j}\cdotx_{i}} + b) \leq 0\)
          <div class="math">$$alpha_{i} = \apha_{i} + \eta$$</div>
<div class="math">$$b = b + {\eta}y_{i}$$</div>
   (4) 转至（2）</p>
</blockquote>
<p>考虑到对偶形式中训练实例仅以内积形式存在，为了方便，我们可以预先将训练集中实例间的内积计算出来并保存</p>
<h4>感知机学习算法对偶形式的python实现</h4>
<p><a href="https://github.com/forin-xyz/statistical-learning-method/blob/master/perceptron_couple.py" title="gihub">感知机学习算法对偶形式</a></p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">perceptron_couple_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">goon</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">Gram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">goon</span><span class="p">:</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="n">Gram</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">while</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="n">Gram</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eta</span>
                    <span class="n">b</span> <span class="o">+=</span> <span class="n">eta</span><span class="o">*</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">break</span>
        <span class="n">goon</span> <span class="o">=</span> <span class="n">flag</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">test_perceptron_couple_train</span><span class="p">():</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">perceptron_couple_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">a</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="kp">all</span><span class="p">()</span> <span class="ow">and</span>
        <span class="p">(</span><span class="n">w</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="kp">all</span><span class="p">()</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">==</span> <span class="o">-</span><span class="mi">3</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;test perceptron_couple_train successful!&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;test perceptron_couple_train failed!&#39;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">test_perceptron_couple_train</span><span class="p">()</span>
</pre></div>


<h2>支持向量机</h2>
<p>感知机模型存在许多可行解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的更好的解，一个很自然的想法就是添加约束条件，使得所有的点到达超平面的距离的最小值最大（间隔最大化）的能够正确划分实例点的分离超平面。于是就得到了线性可分支持向量机。当训练集不可分时，感知机学习算法不收敛，迭代结果会发生震荡，这时候就要对基本的支持向量机进行改造得到其他的支持向量机算法。</p>
<h3>线性可分支持向量机</h3>
<p>给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面</p>
<div class="math">$$w^{*} \cdot x + b ^{*}$$</div>
<p>以及相应的分类决策函数</p>
<div class="math">$$f(x)=sign(w^{*} \cdot x + b)$$</div>
<p>称为线性可分支持向量机。</p>
<h3>几何间隔和函数间隔</h3>
<p>对于给定的超平面(w, b)及其法向量w, 实例点\(x_{i}\)到超平面的距离为:
</p>
<div class="math">$$\hat{\gamma_{i}} = \frac{abs{w \cdot x + b}}{\left \| w \right \|}$$</div>
<p>
亦即
</p>
<div class="math">$$\hat{\gamma_{i}} = \frac{y_{i}(w \cdot x + b)}{\left \| w \right \|}$$</div>
<p>
因为当被正确分类时, \(y_{i}\) 与 \(w \cdot x + b\)的符号是相同的。
取最小的几何间隔为训练数据集到超平面的几何间隔。
这就是几何间隔，当我们不在考虑分类正确与否时，实例点分类错误时几何间隔为负值。</p>
<p>相应的，定义
</p>
<div class="math">$$\hat{\gamma_{i}} = y_{i}(w \cdot x + b)$$</div>
<p>
为实例点\(x_{i}\)到超平面(w, b)的函数间隔。
取所有实例点的函数间隔的最小值为训练数据集到超平面的函数间隔。</p>
<p>我们发现, 函数间隔可以表示分类预测的正确性以及确性度，但是同比例的扩大w和b,就可以得到成比例的函数间隔，但是超平面却没有改变，所以要使得间隔最大化，必需使用几何间隔才能够实现。</p>
<h3>间隔最大化</h3>
<p>于是感知机问题就转化为线性可分支持向量机问题，等价于下面的优化问题:
</p>
<div class="math">$$\begin{aligned}
&amp;\max\limits_{w,b}   &amp;\gamma\\
&amp;s.t.  &amp;y_{i}(\frac{w}{\left \| w \right \|} \cdot x_{i} + \frac{b}{\left \| w \right \|}) \geq \gamma, i=1,2,\cdots,N
\end{aligned}$$</div>
<p>上面的规划问题等价于下面的凸二次规划问题(convex quadratic programming)</p>
<h2>中英文词汇对照表</h2>
<p>感知机 perceptron
线性分类模型 linear classification model
支持向量机 support vector machine, SVM
分离超平面 separating hyperplane
输入空间 input space
输出空间 output space
特征空间 feature space
权值向量 weight vector
偏置 bias
梯度下降法 gradient descent
线性支持向量机 linear support vector machine
非线性支持向量机 non-linear support vector machine
线性可分支持向量机 linear support vector machine in linearly in linearly separable case
硬间隔最大化 hard margin maximization
软间隔最大化 soft margin maximization
核技巧 kenerl trick
核函数 kenerl function
核方法 kenerl method
线性可分数据集 linearly separable data set
损失函数 loss function
经验损失函数 empirical loss function
学习率 learning rate
几何间隔 geometric margin
函数间隔 functional margin
凸二次规划问题 convex quadratic programming</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://forin-xyz.github.io/tutorial.html">我的导航</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://weibo.com/u/1756830393">微博</a></li>
                            <li><a href="http://github.com/forin-xyz">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
        </footer><!-- /#contentinfo -->

</body>
</html>