<!DOCTYPE html>
<html lang="cn">
<head>
        <meta charset="utf-8" />
        <title>耕有田，读有书 - SVM</title>
        <link rel="stylesheet" href="http://forin-xyz.github.io/output/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="http://github.com/forin-xyz/">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="http://forin-xyz.github.io/output/">耕有田，读有书 </a></h1>
                <nav><ul>
                    <li><a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a></li>
                    <li><a href="http://forin-xyz.github.io/output/category/python.html">Python</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-3-luo-ji-hui-gui-mo-xing-yu-zhi-chi-xiang-liang-ji-de-bi-jiao.html">《统计学习方法》读书笔记3-逻辑回归模型与支持向量机的比较, 含python实现</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-02-19T23:00:00+08:00">
                发表于: 2016年02月19日 星期Fri
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/ji-qi-xue-xi.html">机器学习</a> <a href="http://forin-xyz.github.io/output/tag/luo-ji-hui-gui.html">逻辑回归</a> <a href="http://forin-xyz.github.io/output/tag/logistic-regression.html">logistic regression</a> <a href="http://forin-xyz.github.io/output/tag/lr.html">LR</a> <a href="http://forin-xyz.github.io/output/tag/support-vector-machine.html">support vector machine</a> <a href="http://forin-xyz.github.io/output/tag/svm.html">SVM</a> </p>
</footer><!-- /.post-info --><h2>逻辑回归</h2>
<h3>Sigmoid 函数</h3>
<div class="math">$$
g(x) = \frac{1}{1+exp(-x)}
$$</div>
<h3>逻辑斯蒂分布</h3>
<div class="math">$$
F(x) = P(X \leq x) = \frac{1}{1+exp(\frac{-x+\mu}{\gamma})}
$$</div>
<div class="math">$$
f(x) = \frac{exp(\frac{-x+\mu}{\gamma})}{\gamma (1 + exp(\frac{-x+\mu}{\gamma}))^2}
$$</div>
<h3>逻辑斯蒂模型</h3>
<div class="math">$$
P(Y=1|X=x) = g(w \cdot x + b) = \frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)}
$$</div>
<div class="math">$$
P(Y=0|X=x) = 1 - P(Y=1|X=x) = \frac{1}{1+exp(w \cdot x + b)}
$$</div>
<p>等价于
</p>
<div class="math">$$
y^* = g(w \cdot x + b);
$$</div>
<div class="math">$$
y = 1 \qquad if \qquad y^* \geq \frac{1}{2} \qquad else \qquad 0
$$</div>
<p>同时等价于
</p>
<div class="math">$$
y = sign(w \cdot x + b) 如果将类中的0类映射到-1类
$$</div>
<h3>逻辑斯蒂模型的学习 maximum likehood estimation</h3>
<p>令 </p>
<div class="math">$$ w \gets (w^T, b), x \gets (x^T, 1)$$</div>
<div class="math">$$
L(w) = \sum\limits_{i=1}{N}y_{i}\log(g(w \cdot x) + (1 - y_i)log(1-g(w \cdot x)) = \sum\limits_{i=1}{i=N}{y_{i}w \cdot x_i - log(1 + exp(w \cdot x))}
$$</div>
<p>
求上述函数的最大值,等价与求下列函数的极小值
</p>
<div class="math">$$
-\frac{1}{N}L(w) = \frac{1}{N}\sum\limits_{i=1}{N}{log(1 + exp(w \cdot x)) - y_{i}w \cdot x}
$$</div>
<h3>逻辑斯蒂损失</h3>
<div class="math">$$
cost(y_i, x_i) = \log{1 + exp(-y_{i} w \cdot x)} = -\log{g(-y_iw \cdot x_i)}, 若y_i \in {-1, +1}
$$</div>
<p>
相对应的
</p>
<div class="math">$$
当 y_i \in \{0, 1\} 时, cost(x_i, y_i) = log(1 + exp(w \cdot x_i)) - y_{i} w \cdot x
$$</div>
<h3>算法 使用拟牛顿法 BFGS</h3>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span> <span class="nf">costfunction</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">X_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">r_</span><span class="p">[</span><span class="s1">&#39;-1&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="kp">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">l</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="kp">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_</span><span class="p">[</span><span class="n">i</span><span class="p">]))))</span>
    <span class="k">return</span> <span class="n">l</span>

<span class="k">def</span> <span class="nf">costfunction_der</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">X_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">r_</span><span class="p">[</span><span class="s1">&#39;-1&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="kp">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="kp">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">der</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">ywxi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">der_w</span> <span class="o">+=</span> <span class="p">(</span><span class="o">-</span><span class="n">ywxi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span> <span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">ywxi</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">ywxi</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">der_w</span>

<span class="k">def</span> <span class="nf">LR_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># 将该矩阵进行扩展</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="kp">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">zeros</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span>  <span class="n">costfunction</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">cost_der</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">costfunction_der</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">opf</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">cf</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">cost_der</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opf</span><span class="o">.</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">opf</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">LR_predict</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">if</span> <span class="n">f</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>


<h3>多项逻辑回归 multi-nominal logistic regression model</h3>
<div class="math">$$
P(Y=k|x) = \frac{1}{Z(x)}exp(w_k \cdot x)
$$</div>
<div class="math">$$
P(Y=K|x) = \frac{1}{Z(x)}
$$</div>
<div class="math">$$
Z(x) = 1 + \sum\limits_{k=1}^{K-1}{exp(w_k \cdot x)}
$$</div>
<h2>支持向量机 与 逻辑回归的比较</h2>
<h3>相同点</h3>
<ol>
<li>
<p>都是线性模型, 都可以通过<span class="math">\(f(x) = sign(w \cdot x + b)\)</span>来表示</p>
</li>
<li>
<p>损失函数都是0-1损失函数的上界, 且正确的越接近超平面, 其对模型的参数影响越大, 错误的点越远离超平面, 对错误的点影响越大</p>
</li>
</ol>
<h3>不同点</h3>
<ol>
<li>损失函数不同</li>
</ol>
<div class="math">$$
L_{svm}(x, y) = {\left[ 1 - yf(x) \right]}_{+}
$$</div>
<div class="math">$$
L_{lr}(x, y) = log(1 + exp(-yf(x)))
$$</div>
<ol>
<li>
<p>支持向量机可以通过核技巧进行处理非线性问题, 逻辑回归模型的非线性模型比较复杂</p>
</li>
<li>
<p>当数据量非常大时, 逻辑回归的计算量也会非常大, 而支持向量机的计算量却适中</p>
</li>
<li>
<p>当数据量非常小时, 逻辑回归的预测分类效果一般会比支持向量机逊色</p>
</li>
</ol>
<h2>分类模型的评估</h2>
<div class="math">$$
AUG = \frac{\sum\limits_{y_i=1, y_j=-1, i \ne j}{I(G(x_i) = y_i, G(x_j) = y_j)}}{\sum\limits_{y_i=1, y_j=-1, i \ne j}{1}}
$$</div>
<p>在分类误差率相同的情况下, AUG越大的模型其分类效果越好</p>
<h2>扩张疑问</h2>
<p>按照上面的AUG定义, 是否可以通过优化AUG来学习二类分类模型?</p>
<h2>中英文对照表</h2>
<p>逻辑斯蒂分布 logistic distribution</p>
<p>逻辑回归 logistic regression</p>
<p>二项逻辑回归 binomial logistic regression model</p>
<p>对数几率 log odds</p>
<p>多项逻辑回归模型 multi-nominal logistic regression model</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-11-tong-ji-xue-xi-fang-fa-zong-jie.html" rel="bookmark"
                           title="Permalink to 《统计学习方法》读书笔记11-全书小结">《统计学习方法》读书笔记11-全书小结</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2016-02-18T23:00:00+08:00">
                发表于: 2016年02月18日 星期Thu
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/ji-qi-xue-xi.html">机器学习</a> <a href="http://forin-xyz.github.io/output/tag/gan-zhi-ji.html">感知机</a> <a href="http://forin-xyz.github.io/output/tag/perceptron.html">perceptron</a> <a href="http://forin-xyz.github.io/output/tag/zhi-chi-xiang-liang-ji.html">支持向量机</a> <a href="http://forin-xyz.github.io/output/tag/super-vector-machines.html">super vector machines</a> <a href="http://forin-xyz.github.io/output/tag/svm.html">SVM</a> <a href="http://forin-xyz.github.io/output/tag/ji-suan-ji-ke-xue.html">计算机科学</a> <a href="http://forin-xyz.github.io/output/tag/shu-xue-yuan-li.html">数学原理</a> <a href="http://forin-xyz.github.io/output/tag/du-shu-bi-ji.html">读书笔记</a> <a href="http://forin-xyz.github.io/output/tag/tong-ji-xue-xi-fang-fa.html">统计学习方法</a> </p>
</footer><!-- /.post-info -->                <p>《统计学习方法》全书讲了十余个监督学习方法，本节做一个小结。</p>
                <a class="readmore" href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-11-tong-ji-xue-xi-fang-fa-zong-jie.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-2-cong-gan-zhi-ji-dao-zhi-chi-xiang-liang-ji.html" rel="bookmark"
                           title="Permalink to 《统计学习方法》读书笔记2-从感知机到支持向量机，含python源码">《统计学习方法》读书笔记2-从感知机到支持向量机，含python源码</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2016-02-18T23:00:00+08:00">
                发表于: 2016年02月18日 星期Thu
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/ji-qi-xue-xi.html">机器学习</a> <a href="http://forin-xyz.github.io/output/tag/gan-zhi-ji.html">感知机</a> <a href="http://forin-xyz.github.io/output/tag/perceptron.html">perceptron</a> <a href="http://forin-xyz.github.io/output/tag/zhi-chi-xiang-liang-ji.html">支持向量机</a> <a href="http://forin-xyz.github.io/output/tag/super-vector-machines.html">super vector machines</a> <a href="http://forin-xyz.github.io/output/tag/svm.html">SVM</a> <a href="http://forin-xyz.github.io/output/tag/python.html">python</a> <a href="http://forin-xyz.github.io/output/tag/ji-suan-ji-ke-xue.html">计算机科学</a> <a href="http://forin-xyz.github.io/output/tag/shu-xue-yuan-li.html">数学原理</a> <a href="http://forin-xyz.github.io/output/tag/du-shu-bi-ji.html">读书笔记</a> </p>
</footer><!-- /.post-info -->                <p>感知机(perceptron)是二类分类的线性分类模型(linear classfication model)，对应于一个特征空间(feature space)中的一个将实例划分为正负两类的分离超平面(separating hyperplane)。可以使用梯度下降法(gradient descent)对感知机模型的损失函数进行极小化求出模型参数。支持向量机(support vector machines, SVM)也是二类分类模型，线性支持向量机(linear support vector machine)与感知机(perceptron)不同在于其使间隔最大化。通过核技巧(kernel trick)以及软间隔最大化(soft margin maximization),可以学习非线性支持向量机(non-linear support vector machine)。支持向量机的学习算法就是求解凸二次规划的最优化算法。</p>
                <a class="readmore" href="http://forin-xyz.github.io/output/2016/02/du-shu-bi-ji-2-cong-gan-zhi-ji-dao-zhi-chi-xiang-liang-ji.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://forin-xyz.github.io/tutorial.html">我的导航</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://weibo.com/u/1756830393">微博</a></li>
                            <li><a href="http://github.com/forin-xyz">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
        </footer><!-- /#contentinfo -->

</body>
</html>