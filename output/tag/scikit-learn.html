<!DOCTYPE html>
<html lang="cn">
<head>
        <meta charset="utf-8" />
        <title>耕有田，读有书 - scikit-learn</title>
        <link rel="stylesheet" href="http://forin-xyz.github.io/output/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="http://github.com/forin-xyz/">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="http://forin-xyz.github.io/output/">耕有田，读有书 </a></h1>
                <nav><ul>
                    <li><a href="http://forin-xyz.github.io/output/category/ji-qi-xue-xi.html">机器学习</a></li>
                    <li><a href="http://forin-xyz.github.io/output/category/python.html">Python</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="http://forin-xyz.github.io/output/2016/03/scikit-learnchang-yong-ji-qi-xue-xi-fang-fa.html">scikit-learn常用机器学习方法</a></h1>
<footer class="post-info">
        <abbr class="published" title="2016-03-05T10:20:00+08:00">
                发表于: 2016年03月05日 星期Sat
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/python.html">Python</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/scikit-learn.html">scikit-learn</a> <a href="http://forin-xyz.github.io/output/tag/ji-qi-xue-xi.html">机器学习</a> </p>
</footer><!-- /.post-info --><h2>数据导入</h2>
<p>可以使用numpy的loadtxt方法导入csv文件</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="c1"># fn是文件路径 或者 是拥有read方法的对象实例</span>
<span class="n">np</span><span class="o">.</span><span class="kp">loadtxt</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
</pre></div>


<h2>数据预处理, 数据标准化</h2>
<h3>StandardScaler</h3>
<p>使训练集每一个特征值<strong>0-均值,1-方差</strong>, 作用在训练集的列(column)上</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="n">standard_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="c1"># 训练并转换数据集</span>
<span class="n">Xt</span> <span class="o">=</span> <span class="n">standard_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 转换新的数据集</span>
<span class="n">standard_scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 原训练集均值</span>
<span class="n">standard_scaler</span><span class="o">.</span><span class="n">mean_</span>
<span class="c1"># 原训练集标准差</span>
<span class="n">standard_scaler</span><span class="o">.</span><span class="n">std_</span>
</pre></div>


<h3>MinMaxScaler, MaxAbsScaler</h3>
<p>指定的极差, 默认min=0, max=1, max_abs=1</p>
<div class="highlight"><pre><span></span>min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit(X)
min_max_scaler.transform(x)
# min_, 原训练集各个特征最小值, scale_, 原训练集各个特征的范围(scale = max-min)
min_max_scaler.min_
min_max_scaler.scale_
</pre></div>


<p>对于<strong>稀疏矩阵(sparse matrices)</strong>非常有效</p>
<h3>Normalizer</h3>
<p>使每一个样本具有<strong>单位范数, norm=1</strong>, 作用在训练集的行(row)上</p>
<div class="highlight"><pre><span></span># norm in `{l1, l2, max}`, default norm=&#39;l2&#39;
normalizer = preprocessing.Normalizer(norm=&#39;l1&#39;)
normalizer.fit(X)
normalizer.transform(X)
</pre></div>


<p><strong>小写的scale, minmax_scale, maxabs_scale, normalize是方法, 可以直接将数据集标准化, 返回值就是转化后的新数据集, 但是不能够对新的数据按照相同的参数进行转化</strong></p>
<h2>特征提取</h2>
<h2>特征降维, 主成分分析PCA, 独立成分分析ICA, 因子分析FA</h2>
<h3>PCA, principal component analysis</h3>
<p>只对符号高斯分布的样本点比较有效</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="c1"># n_components : int, 向量个数</span>
<span class="c1"># float: 0&lt;n&lt;1, the amount of variance that needs to greater than n.</span>
<span class="c1"># &#39;mle&#39;: 极大似然估计</span>
<span class="c1"># None: min(n_samples, n_features)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">decompositon</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 最大方差的方向, 特征空间的主要坐标轴</span>
<span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
<span class="c1"># explained_variance_ratio_ : 所选取的向量对应的方差的百分数</span>
<span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="c1"># mean_: 每个特征的经验均值</span>
<span class="n">pca</span><span class="o">.</span><span class="n">mean_</span>
<span class="c1"># n_components_: 模型提取的特征数</span>
<span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>
<span class="c1"># 转换数据集数据</span>
<span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 返回对数似然函数在样本上的平均值</span>
<span class="n">pca</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>


<h3>ICA, independent component analysis</h3>
<h3>FA</h3>
<h2>KNN, k-近邻</h2>
<h3>NearestNeighbors, 返回最近的邻居</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="n">nbrs</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 距离每个样本最近的样本点的索引</span>
<span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">kdt</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<h3>KDTree, kd树</h3>
<div class="highlight"><pre><span></span>kdt = neighbors.KDTree(X, leaf_size=30, metric=&#39;euclidean&#39;)
</pre></div>


<h3>KNeighborsClassifier, kNN分类器</h3>
<div class="highlight"><pre><span></span>neigh = neighbors.KNeighborsClassifier(n_neighbors=3)
neigh.fit(X)
neigh.predict(Xs)
# freq(类)/k, 返回值是一个向量, 象征每个类在的k近邻经验概率分布
neigh.predict_proba(Xt)
# Returns the mean accuracy on the given test data and labels.
neigh.score(Xv, yv)
</pre></div>


<h2>NB, 朴素贝叶斯</h2>
<h3>一般朴素贝叶斯方法, MultinomialNB</h3>
<p>特征向量分量是离散变量时很有用</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">naive_bayes</span>
<span class="n">nb</span> <span class="o">=</span> <span class="n">naive_bayes</span><span class="o">.</span><span class="n">MutilnomialNB</span><span class="p">()</span>
<span class="n">nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>


<p><strong><span class="math">\(\lambda = 0\)</span>是代表极大似然估计, <span class="math">\(\lambda = 1\)</span>是代表拉普拉斯平滑, <span class="math">\(\lambda &gt; 0\)</span>时是贝叶斯估计.</strong></p>
<h3>高斯朴素贝叶斯方法, GaussianNB</h3>
<p>假设特征向量分量关于Y的条件分布符合正态分布, 即<span class="math">\(P(X^i|Y)\)</span>是正态分布, 对X是连续变量时有用</p>
<div class="highlight"><pre><span></span>gnb = naive_bayes.GaussianNB()
gnb.fit(X)
</pre></div>


<h3>伯努利朴素贝叶斯方法， BernoulliNB</h3>
<p>假设特征向量分量关于Y的条件分布是伯努利分布</p>
<div class="highlight"><pre><span></span>bnb = naive_bayes.BernoulliNB(alpha=1.0)
bnb.fit(X)
</pre></div>


<p><strong><span class="math">\(\lambda = 0\)</span>是代表极大似然估计, <span class="math">\(\lambda = 1\)</span>是代表拉普拉斯平滑, <span class="math">\(\lambda &gt; 0\)</span>时是贝叶斯估计.</strong></p>
<h2>LR, 逻辑回归分类模型</h2>
<div class="math">$$
\min\limits_{w, b}{\frac{1}{2} {\lVert w \rVert}^2 + C \sum\limits_{i=1}{N}{log{(1+exp(-y_i(w \cdot x_i + b)))}}}
$$</div>
<p>
或者等价于
</p>
<div class="math">$$
\min\limits_{w, b}{\sum\limits_{i=1}^{N}{\log{(1+exp(-y_i(w \cdot x_i + b)))}} + \lambda {\lVert w \rVert}^2}
$$</div>
<p>
如果penalty='l1', 就将上面公式中的l2范数换成l1范数即可, 即各分量绝对值的和</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="c1"># penalty -&gt; 范数 or 惩罚函数,default=&#39;l2&#39;, C 惩罚系数, log(exp(-yi(w*xi+b))+1)的系数</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogsiticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="c1"># coef_ -&gt; w, intercept_ -&gt; b</span>
</pre></div>


<h2>SVM, 支持向量机</h2>
<h3>线性支持向量机</h3>
<div class="math">$$
\min\limits_{w, b}{\frac{1}{2} {\lVert w \rVert}^2 + C \sum\limits_{i=1}^{N}{[1-y_i(w \cdot x_i + b)]_{+}}}
$$</div>
<p>
或者等价于
</p>
<div class="math">$$
\min\limits_{w, b}{\sum\limits_{i=1}^{N}{[1-y_i(w \cdot x_i + b)]_{+}} + \lambda {\lVert w \rVert}^2}
$$</div>
<p>
如果penalty='l1', 就将上面公式中的l2范数换成l1范数即可, 即各分量绝对值的和</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="c1"># loss in [&#39;hingle&#39;, &#39;squard_hingle&#39;], default=&#39;hingle&#39;</span>
<span class="n">linear_svm</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;hingle&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)</span>
</pre></div>


<h3>非线性支持向量机</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="c1"># kernel is in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable], default=&#39;rbf&#39;</span>
<span class="c1"># degree: degree for &#39;poly&#39;, ignored by all other method, default=3</span>
<span class="c1"># gamma: &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;核函数系数, 如果选择&#39;auto&#39;的话, 1/n_features将被选择</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">de</span><span class="p">)</span>
</pre></div>


<h2>ME, 最大熵模型</h2>
<h2>DT, 决策树</h2>
<p>scikit-learn使用的是优化的CART算法</p>
<h3>分类树</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="c1"># criterion: &#39;gini&#39;, &#39;entropy&#39;, default=&#39;gini&#39;</span>
<span class="n">dtc</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassfier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">)</span>
</pre></div>


<h3>回归树</h3>
<div class="highlight"><pre><span></span># criterion: &#39;mse&#39;, default=&#39;mse&#39;, mean squared error
dtr = tree.DecisionTreeRegressor(criterion=&#39;mse&#39;)
</pre></div>


<h2>BT, 提升树, Gradient Tree Boosting</h2>
<h3>分类, GradientBoostingClassifier</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">ensemble</span>
<span class="n">gbc</span> <span class="o">=</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<h3>回归, GradientBoostingRegressor</h3>
<div class="highlight"><pre><span></span>gbr = ensemble.GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss=&#39;ls&#39;).fit(X_train, y_train)
</pre></div>


<h2>AdaBoost, 提升方法</h2>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="n">abclf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">abclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<h2>HMM, 隐马尔科夫模型</h2>
<h2>CRF, 条件随机场</h2>
<h2>CV, 交叉验证</h2>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="c1"># cross_val_score, cross_val_predict</span>
<span class="c1"># 默认使用k-fold验证</span>
<span class="c1"># array-like, shape[0] = len(list(cv)) if type(cv) is not int else cv</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># shape=(n_samples, )</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_predict</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
</pre></div>


<h3>K-fold</h3>
<div class="highlight"><pre><span></span><span class="c1"># n -&gt; int, 样本的总数量, n_folds &lt;- number of folds, default=3, at least 2.</span>
kf <span class="o">=</span> cross_validation.KFold<span class="p">(</span><span class="m">4</span><span class="p">,</span> n_folds<span class="o">=</span><span class="m">2</span><span class="p">)</span>
<span class="c1"># train, test 是训练样本和验证样本的索引构成的列表</span>
<span class="kr">for</span> train<span class="p">,</span> test <span class="kr">in</span> kf<span class="o">:</span>
    <span class="p">(</span>train<span class="p">,</span> test<span class="p">)</span>
</pre></div>


<h3>LOO, Leave-One-Out</h3>
<div class="highlight"><pre><span></span># n -&gt; int, the total of elements
loo = cross_validation.LeaveOneOut(8)
for train, test in lo:
    (train, test)
</pre></div>


<h3>LPO, Leave-P-Out</h3>
<div class="highlight"><pre><span></span>lpo = cross_validation.LeavePOut(n, p)
</pre></div>


<p><strong>这些交叉验证的模型可以代入最上面的</strong><code>cross_val_score</code>, <code>cross_val_predict</code><strong>方法中</strong></p>
<h3>random permutations cross validation</h3>
<div class="highlight"><pre><span></span>ss = cross_validation.ShuffleSplit(5, n_iter=3, test_size=0.25, random_state=0)
</pre></div>


<h2>高斯混合模型</h2>
<p>非监督学习模型, unsupervised learning</p>
<div class="math">$$
P(y|\sigma) = \sum\limits_{k=1}^{K}{\alpha_{k}\phi(y|\sigma_{k})}
$$</div>
<div class="math">$$
{\phi}(y|\sigma_{k}) = \frac{1}{(2 \pi)^{\frac{p}{2}}{\sqrt(det({COVMAT}_k))}}exp(-\frac{1}{2}{(y-{\mu}_k)^T({COVMAT}_k)^{-1}(y-{\mu}_k)});
$$</div>
<p>
其中, <span class="math">\({COVMAT}_k\)</span>是 (n_features, n_features) 的矩阵, <span class="math">\({\mu}_k\)</span>是 (n_features, )向量</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">mixture</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 均值</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">means_</span>
<span class="c1"># 协方差</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">covars_</span>
<span class="c1"># 权重</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span>
<span class="c1"># 预测</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">XP</span><span class="p">)</span>
</pre></div>


<p>高斯混合模型首先通过观测数据使用EM算法计算出模型的参数<span class="math">\(\sigma\)</span>, 然后通过计算出在给定的参数下, 观测数据最有可能通过第几个模型产生, 达到对数据进行分类的目的.</p>
<p>如果将其他概率密度替代高斯混合模型中的高斯分布密度, 那么就会得到相应的概率混合模型, 然后可以使用类似办法对类进行分类.</p>
<h2>聚类, Clustering</h2>
<h3>K-Means, k-均值, KMeans, MiniBatchKMeans</h3>
<p>输入参数: 聚类的个数</p>
<div class="math">$$
\min\sum\limits_{i=1}^{N}{\min\limits_{k}{\lVert x_i-{\mu}_k \rVert}^2}
$$</div>
<p><strong>MiniBatchKMeans</strong>是KMeans的变种, 可以减少运行时间.</p>
<h3>MeanShift, 将中心点逐渐移向密度最稠密的点</h3>
<p>scikit-learn使用的是<code>rbf kernel</code>, 高斯径向基函数.</p>
<h3>DBSCAN</h3>
<p>高密度区域天然被低密度区域分割</p>
<h2>二聚类, Biclustering</h2>
<h2>模型持久化</h2>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">joblib.externals</span> <span class="kn">import</span> <span class="n">joblib</span>
<span class="c1"># 保存</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span><span class="s1">&#39;../../data/model/randomforest.pkl&#39;</span><span class="p">,</span><span class="n">compress</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># 加载</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;../../data/model/randomforest.pkl&#39;</span><span class="p">)</span>
</pre></div>


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="http://forin-xyz.github.io/output/2016/03/scikit-learnjian-yao.html" rel="bookmark"
                           title="Permalink to scikit-learn简要">scikit-learn简要</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2016-03-03T23:00:00+08:00">
                发表于: 2016年03月03日 星期Thu
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://forin-xyz.github.io/output/author/forin-xyz.html">forin-xyz</a>
        </address>
<p>In <a href="http://forin-xyz.github.io/output/category/python.html">Python</a>.</p>
<p>tags: <a href="http://forin-xyz.github.io/output/tag/python.html">python</a> <a href="http://forin-xyz.github.io/output/tag/scikit-learn.html">scikit-learn</a> <a href="http://forin-xyz.github.io/output/tag/machine-learning.html">machine learning</a> <a href="http://forin-xyz.github.io/output/tag/supervised-learning.html">supervised learning</a> <a href="http://forin-xyz.github.io/output/tag/non-supervised-learning.html">non-supervised learning</a> <a href="http://forin-xyz.github.io/output/tag/classfication.html">classfication</a> <a href="http://forin-xyz.github.io/output/tag/prediction.html">prediction</a> <a href="http://forin-xyz.github.io/output/tag/clustering.html">clustering</a> <a href="http://forin-xyz.github.io/output/tag/dimensionality.html">dimensionality</a> </p>
</footer><!-- /.post-info -->                <p>scikit-learn是python重要的机器学习和数据挖掘的软件包.</p>
                <a class="readmore" href="http://forin-xyz.github.io/output/2016/03/scikit-learnjian-yao.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://forin-xyz.github.io/tutorial.html">我的导航</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://weibo.com/u/1756830393">微博</a></li>
                            <li><a href="http://github.com/forin-xyz">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
        </footer><!-- /#contentinfo -->

</body>
</html>