Title: 《统计学习方法》读书笔记11-全书小结
Date: 2016-03-09 23:08
Category: 机器学习
Tags: 机器学习, 感知机, perceptron, 支持向量机, super vector machines, SVM, 计算机科学, 数学原理, 读书笔记, 统计学习方法
Summary: 《统计学习方法》全书讲了十余个监督学习方法，本节做一个小结。

# 感知机

## 损失函数

$$
\min\limits_{w, b}{\sum\limits_{i=1}^{N}[-y_i(w \cdot x_i + b)]_{+}}
$$

## 原始形式

如果$y_i(w \cdot x_i + b) \leq 0$
$$\begin{aligned}
w \gets w + \eta y_i x_i\\
b \gets b + \eta y_i
\end{aligned}
$$

## 对偶形式

如果$y_i( \sum\limits_{j=1}^{N}(x_j \cdot x_i) + b) \leq 0$
$$\begin{aligned}
{\alpha}_i \gets {\alpha}_i + \eta\\
b \gets b + \eta y_i
\end{aligned}
$$

## 算法收敛性

**Novikoff定理**

# 逻辑回归模型 LR

## 原始形式

$$
\min\limits_{w, b} \sum\limits_{i=1}^{N}\log{(1 + exp(-y_i (w \cdot x_i + b)))} + \lambda {\lVert w \rVert}_{2}^2
$$

# 支持向量机

## 线性可分支持向量机 -- 硬间隔最大化

### 原始形式
$$
\begin{aligned}
\min\limits_{w, b} \frac{1}{2}{\lVert w \rVert}_{2}^2\\
s.t.\\
y_i (w \cdot x_i + b ) \leq 1
\end{aligned}
$$

### 对偶形式 dual
$$
\min\limits_\alpha \sum\limits_{i=1}^{N}\sum\limits_{i=1}^{N}{\alpha_i{\alpha}_j y_i y_j x_i \cdot x_j - \sum\limits_{i=1}^{N}\alpha _i}
$$
subject to
$$
\sum\limits_{i=1}{N} = 0
$$
$$
\alpha _i \geq 0
$$

其中
$$
w = \sum\limits_{i=1}^{N} {\alpha}_i y_i x_i
$$
当${\alpha}_i \gt 0$
$$
b = y_j - \sum\limits_{i=1}^{N}{y_i \alpha _i (x_i \cdot x_j)}
$$


## 线性支持向量机 -- 软间隔最大化

### 原始形式
$$
\begin{aligned}
\min\limits_{w, b} \frac{1}{2}{\lVert w \rVert}_{2}^2 + C\sums\limits_{i=1}^{N}[1-y_i (w \cdot x_i + b)]_{+}
\end{aligned}
$$
或者
$$
\min\limits_{w, b} \sums\limits_{i=1}^{N}[1-y_i (w \cdot x_i + b)]_{+} + \lambda {\lVert w \rVert}_{2}^2
$$

### 对偶形式 dual
$$
\min\limits_{\alpha} \sum\limits_{i=1}^{N}\sum\limits_{i=1}^{N}{\alpha_i{\alpha}_j y_i y_j x_i \cdot x_j - \sum\limits_{i=1}^{N}{\alpha _i}}
$$
subject to
$$
\sum\limits_{i=1}{N} = 0
$$
$$
0 \leq \alpha _i \leq C
$$

## 非线性支持向量机 -- 核技巧

$$
\min\limits_{\alpha} \sum\limits_{i=1}^{N}\sum\limits_{i=1}^{N}{\alpha_i{\alpha}_j y_i y_j K(x_i, x_j) - \sum\limits_{i=1}^{N}{\alpha _i}}
$$
subject to
$$
\sum\limits_{i=1}{N} = 0
$$
$$
0 \leq \alpha _i \leq C
$$

# 最大熵模型

$$
P_w(y|x) = \frac{exp(\sum\limits_{i=1}^{n} w_i f_i(x, y))}{Z_w(x)}
$$
其中$Z_w(x) = \sum\limits_{y}exp(\sum\limits_{i=1}^{n} w_i f_i(x, y))$

## 对偶形式

$$
\max\limits_{w} \sum\limits_{x, y} \widetilde{P}(x, y)\sum\limits_{i=1}{n} w_i f_i(x, y) - \sum\limits_{x}\widetilde{P}(x)\log{Z_w(x)}
$$

## 改进的迭代尺度法 IIS, improved iterative scaling

# kNN

找出实例点在训练集离其最近的k个点组成一个集合, 然后使用给定的分类决策规则如多数表决决定该实例点的类别

# kd树 -- 存储k维空间数据的树结构

# NB

## 条件独立性假设 -- 类给定的条件实例点特征互相独立

## 模型

$$
y = \arg \max \limits_{c_k} P(Y=c_k)\prod _{j}P(X^(j) = x^(j)|Y=c_k)
$$

## 高斯朴素贝叶斯

假设$P(X^(j)|Y=c_k)$满足高斯分布

## 伯努利朴素贝叶斯

当各特征是二值化时

## 一般情况

$$
P(X^(j)=x^(j)|Y=c_k) = \frac{freq(x^(j), c_k) + \lambda}{freq(c_k) + S_j \lambda}
$$
$$
P(Y=c_k) = \frac{freq(c_k) + \lambda}{N + K \lambda}
$$


# DT

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

## ID3

使用信息增益来选择最优特征

## C4.5

使用信息增益比选择最优特征

## 决策的减枝

结构风险最小化的过程

## CART, 分类和回归树

二叉树

回归树使用 平方误差最小化 准则，

分类树使用 Gini指数

# Boost

## AdaBoost

## 加法模型

## 提升树

# EM algorithm

## Q函数

## F函数

## 高斯混合模型

# HMM

## 概率计算方法

## 监督学习方法

## 预测算法

# CRF
