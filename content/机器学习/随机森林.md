Title: 随机森林介绍
Date: 2016-03-16 23:08
Category: 机器学习
Tags: 随机森林， random forest, RF
Summary: 随机森林算法是多个决策树的组合。

# 介绍

随机森林,顾名思义, 随机指的是样本随机，特征随机， 森林指的是多个决策树的组合， 连在一起，就是将多次随机的抽取样本和特征生成的决策树进行简单的组合而成。

# 具体算法

输入: 训练集T, 样本个数为N, 特征个数为M, 抽样选取的特征数为m($m \leq M$), 需要的树的个数K.
输出: 随机森林


1 for k = 0 to K:

  + 从样本T中有放回的随机抽取N个新的训练集

  + 从特征集中无放回的选取m个特征

  + 利用新得到的训练集和特征集训练产生一个完全树，不进行减枝

2 将这些得到的树组合在一起就得到了随机森林，对于一个新的数据，直接用每一颗树对其进行预测，然后选取被选择的最多的类作为其的预测。

# 随机森林的应用

## 用来进行特征选择

#### 平均正确率减少 Mean decrease accuracy

打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。

`np.random.shuttle(X[:, i])`函数可以打乱第i个特征值的顺序

#### 平均不纯度减少 mean decrease impurity

利用不纯度可以确定节点（最优条件），对于分类问题，通常采用 基尼不纯度 或者 信息增益 ，对于回归问题，通常采用的是 方差 或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。


# 优缺点

## 优点

随机森林具有准确率高、鲁棒性好、易于使用、训练速度快、 实现比较简单、易并行、、不易过拟合、通用性强等优点。

任何一个监督学习的问题，使用随机森林，总可以得到一个还不错的模型。

## 缺点

由于过大的规模，模型及其容易消耗大量的内存。

因为测试集是随机产生，所以模型不稳定。
