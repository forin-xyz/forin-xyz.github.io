Title: 《统计学习方法》读书笔记8-k近邻法及python实现
Date: 2016-02-28 23:00
Category: 机器学习
Tags: 机器学习, k近邻法, k-nearest neighbor method, k-NN, kd树, 读书笔记
Summary: k近邻法是一种基本的分类和回归方法。它通过计算所有样本点到待预测点的距离, 选出距离最小的k个样本点, 然后根据这k个样本的类进行多数表决来预测待预测点所属的类。

## k近邻算法

** k 近邻法 **
>输入: 训练数据集T, 其中特征空间$\mathcal{X} \subseteq \boldsystem{R}^n$, 输出空间$\mathcal{Y} \subseteq \{c_1, c_2, \cdots, c_K\}$.
>
>待预测实例特征向量$x$ 以及参数k
>
>输出: 实例$x$所属的类$y$
>
>(1) 在训练集T中找出与x最邻近的k个点构成几何$N_k(x)$
>
>(2) 在集合$N_k(x)$中根据多数表决决定x所属的类
>$$
>y = \arg \max \limits_{c_j}{ freq(y = c_j, (x_i, y_i) \in N_k(x))}
>$$

当k=1时, 称为最近邻算法，此时将训练数据集中与x最近邻的点的类作为x的类。

### k近邻算法的python实现

[k近邻算法](https://github.com/forin-xyz/statistical-learning-method/blob/master/k_nearest_neighbor.py "gihub")

    import numpy as np
    
    def kNN_train(X, y, x, k):
        X = np.array(X)
        y = np.array(y)
        x = np.array(x)
        yk = y[np.argsort(((X-x)**2).sum(axis=1)) < k]
        yunique = np.unique(yk)
        yend = yunique[(yk - yunique[:, np.newaxis]).sum(axis=1).argmax()]
        return yend
    
    def test_kNN_train():
        X = [
            [1, 2],
            [2, 3],
            [3, 4],
            [5, 5],
            [8, 9],
            [4, 3]
        ]
        y = [
            1, 1, 1, -1, -1, -1
        ]
        x = [4.5, 4.5]
        ypredit = kNN_train(X, y, x, 3)
        if ypredit == -1:
            print('test kNN_train sucessfully')
        else:
            print('test kNN_train failed!')
    
    if __name__ == '__main__':
        test_kNN_train()


## k近邻模型

### 距离度量

$L_p$距离　或者　Ｍinkowski距离:
$$
L_p(x_i, x_j) = (\sum\limits_{l=1}{n}{\abs{x_{i}^{l} - x_{j}^{l}}^p})^{\frac{1}{p}}
$$
这里$p \geq 1$

欧式距离 Euclidean distance:
$$
L_2(x_i, x_j) = (\sum\limits_{l=1}{n}{\abs{x_{i}^{l} - x_{j}^{l}}^2})^{\frac{1}{2}}
$$

曼哈顿距离 Manhattan distance
$$
L_2(x_i, x_j) = \sum\limits_{l=1}{n}{\abs{x_{i}^{l} - x_{j}^{l}}}
$$

当$p=\infty$时,
$$
L_{\infty}(x_i, x_j) = \max\limits_{l}{\abs{x_{i}^{l} - x_{j}^{l}}}
$$

### k的选择

如果选择较小的k值,就相当于用较小的邻域中的训练实例进行预测,"学习"的近似误差(approximation error)会减小, 但缺点是"学习"的估计误差(estimation error)会增大,预测结果会对近邻的实例点敏感.
如果邻近的实例点恰巧是噪声,预测就会出错.
换句话说,k值的减小就意味着整体模型变得复杂,容易产生过拟合

如果选择较大的k值,就相当于用较大的邻域进行预测,其优点是可以减小学习的估计误差,但缺点是学习的近似误差会增大. 这时与实例脚癣的训练实例也会对预测起作用,使预测发生错误,k值的增大意味着整体的模型变得简单.

若k=N, 那么无论输入实例是什么, 它都会简单的预测它属于训练中最多的类.

### 分类决策规则

多数表决等价于风险函数为0-1损失函数的经验风险最小化

## k近邻法的实现:kd树

**算法2 构造平衡kd树**
>输入: k维空间数据集T
>
>输出: kd树
>
>(1) 开始: 构造根节点,根节点对应于包含T的k维空间的超矩形区域
>
>选择$x^{(1)}$为坐标轴, 以T中所有实例点的$x^{(1)}$坐标的中位数为切分点,将根结点对应的超矩形区域切分成两个子区域.切分是由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现
>
>由根结点生成深度为1的左右子结点:左子结点对应于$x_{(1)}$坐标小于切分点的子区域,右子结点对应于$x_{(1)}$坐标大于切分点的子区域, 并将落在切分超平面上的实例点保存在根节点中.
>
>(2) 重复: 对深度为j的结点, 选择$x^{(l)}$为切分的坐标轴, $l = j \mod k + 1$.
>
>(3) 直到两个子区域没有实例存在时停止,从而构成kd树区域的划分

注意,这里的中位数在样本数量为偶数时,是取最中间的两个数较大的那个数

### kd树的搜索

**算法3 用kd树的k近邻算法**
>输入: 已构造的kd树, 目标点$x$, k值
>
>输出: x的k近邻构成的集合
>
>(1) 在kd树中找出包含目标点k的叶结点: 从根结点开始,递归的向下访问kd树. 若目标点x当前维的坐标小于切分点的坐标, 则移动至左子结点, 否则移动到右子结点, 直到子结点为叶结点为止
>
>(2) 从结点向上回退, 直到相应的结点包含的实例点个数大于等于k, 从这些实例点中选取出离目标实例点最近的k个实例点为x的"k近邻"$N_k(x)$
>
>(3) 继续递归的像父结点回退, 并对每个结点进行以下操作:
>
>(a) 如果该结点保存的实例点含有比当前"k近邻"集合$N_k(x)$中的点离目标点更近的实例点, 则将$N_k{x}$中离目标点较远的点给替换成较近的点
>
>(b) 接着检查该子结点的父结点对应的另一子节点对应的区域是否含有更近点,如果有,则向上面一样对$N_k(x)$进行更新
>
>(4) 当回退到根结点时,搜索结束. 最后的$N_k(x)$即为所求的集合.

如果实例点是随机分布的, 那么kd树搜索的平均计算复杂度为$O(\log{N})$, kd树适合于训练实例数远大于空间维数时的k近邻搜索.

### 使用kd树的k近邻算法的python实现

## 中英文对照表

k近邻 k-nearest neighbor, k-NN
单元　cell
类标记　class label
距离　distance
欧式距离 Euclidean distance
近似误差 approximation error
估计误差 estimation error
kd树 kd tree
线性扫描 linear scan
划分 partition
中位树 median

