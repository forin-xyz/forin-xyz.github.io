Title: 《统计学习方法》读书笔记3-逻辑回归模型与支持向量机的比较, 含python实现
Date: 2016-02-19 23:00
Category: 机器学习
Tags: 机器学习, 逻辑回归, logistic regression, LR, support vector machine, SVM
Summary: 逻辑回归模型是统计学习中的经典分类方法, 属于对数分类模型。

## 逻辑回归

### Sigmoid 函数

$$
g(x) = \frac{1}{1+exp(-x)}
$$

### 逻辑斯蒂分布

$$
F(x) = P(X \leq x) = \frac{1}{1+exp(\frac{-x+\mu}{\gamma})}
$$
$$
f(x) = \frac{exp(\frac{-x+\mu}{\gamma})}{\gamma (1 + exp(\frac{-x+\mu}{\gamma}))^2}
$$

### 逻辑斯蒂模型
$$
P(Y=1|X=x) = g(w \cdot x + b) = \frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)}
$$
$$
P(Y=0|X=x) = 1 - P(Y=1|X=x) = \frac{1}{1+exp(w \cdot x + b)}
$$

等价于
$$
y^* = g(w \cdot x + b);
$$
$$
y = 1 \qquad if \qquad y^* \geq \frac{1}{2} \qquad else \qquad 0
$$

同时等价于
$$
y = sign(w \cdot x + b) 如果将类中的0类映射到-1类
$$

### 逻辑斯蒂模型的学习 maximum likehood estimation
令 $$ w \gets (w^T, b), x \gets (x^T, 1)$$
$$
L(w) = \sum\limits_{i=1}{N}y_{i}\log(g(w \cdot x) + (1 - y_i)log(1-g(w \cdot x)) = \sum\limits_{i=1}{i=N}{y_{i}w \cdot x_i - log(1 + exp(w \cdot x))}
$$
求上述函数的最大值,等价与求下列函数的极小值
$$
-\frac{1}{N}L(w) = \frac{1}{N}\sum\limits_{i=1}{N}{log(1 + exp(w \cdot x)) - y_{i}w \cdot x}
$$

### 逻辑斯蒂损失

$$
cost(y_i, x_i) = \log{1 + exp(-y_{i} w \cdot x)} = -\log{g(-y_iw \cdot x_i)}, 若y_i \in {-1, +1}
$$
相对应的
$$
当 y_i \in \{0, 1\} 时, cost(x_i, y_i) = log(1 + exp(w \cdot x_i)) - y_{i} w \cdot x
$$

### 算法 使用拟牛顿法 BFGS

    import numpy as np
    from scipy.optimize import minimize
    
    def costfunction(w, X, y):
        N = y.shape[0]
        X_ = np.r_['-1', X, np.ones(N).reshape(N, 1)]
        l = 0
        for i in range(N):
            l += np.log(1 + np.e ** (- y[i] * (np.dot(w, X_[i]))))
        return l
    
    def costfunction_der(w, X, y):
        N = y.shape[0]
        X_ = np.r_['-1', X, np.ones(N).reshape(N, 1)]
        der = np.zeros_like(w)
        for i in range(N):
            ywxi = y[i] * np.dot(w, X_[i])
            der_w += (-ywxi) * np.e **(-ywxi) * y[i] * X_[i] / (1 + np.e ** (-ywxi))
        return der_w
    
    def LR_train(X, y):
        # 将该矩阵进行扩展
        m = X.shape[1]
        w0 = np.zeros(m+1)
        cf = lambda w:  costfunction(w, X, y)
        cost_der = lambda w: costfunction_der(w, X, y)
        opf = minimize(cf, w0, method='BFGS', jac=cost_der)
        return opf.x[:-1], opf.x[-1]
    
    def LR_predict(w, b, x):
        f = np.dot(w, x) + b
        if f >= 0:
            return 1
        else:
            return -1


### 多项逻辑回归 multi-nominal logistic regression model

$$
P(Y=k|x) = \frac{1}{Z(x)}exp(w_k \cdot x)
$$

$$
P(Y=K|x) = \frac{1}{Z(x)}
$$

$$
Z(x) = 1 + \sum\limits_{k=1}^{K-1}{exp(w_k \cdot x)}
$$

## 支持向量机 与 逻辑回归的比较

### 相同点

1. 都是线性模型, 都可以通过$f(x) = sign(w \cdot x + b)$来表示

2. 损失函数都是0-1损失函数的上界, 且正确的越接近超平面, 其对模型的参数影响越大, 错误的点越远离超平面, 对错误的点影响越大

### 不同点

1. 损失函数不同

$$
L_{svm}(x, y) = {\left[ 1 - yf(x) \right]}_{+}
$$
$$
L_{lr}(x, y) = log(1 + exp(-yf(x)))
$$

2. 支持向量机可以通过核技巧进行处理非线性问题, 逻辑回归模型的非线性模型比较复杂

3. 当数据量非常大时, 逻辑回归的计算量也会非常大, 而支持向量机的计算量却适中

4. 当数据量非常小时, 逻辑回归的预测分类效果一般会比支持向量机逊色

## 分类模型的评估

$$
AUG = \frac{\sum\limits_{y_i=1, y_j=-1, i \ne j}{I(G(x_i) = y_i, G(x_j) = y_j)}}{\sum\limits_{y_i=1, y_j=-1, i \ne j}{1}}
$$

在分类误差率相同的情况下, AUG越大的模型其分类效果越好

## 扩张疑问

按照上面的AUG定义, 是否可以通过优化AUG来学习二类分类模型?

## 中英文对照表

逻辑斯蒂分布 logistic distribution

逻辑回归 logistic regression

二项逻辑回归 binomial logistic regression model

对数几率 log odds

多项逻辑回归模型 multi-nominal logistic regression model
