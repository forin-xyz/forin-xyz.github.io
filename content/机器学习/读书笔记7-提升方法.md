Title: 《统计学习方法》读书笔记7-提升方法，含python实现代码
Date: 2016-02-24 23:00
Category: 机器学习
Tags: 机器学习, 提升方法, 读书笔记, AdaBoost, 提升树, boosting tree
Summary: 提升方法是一种常用的统计学习方法, 它通过改变样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。AdaBoost算法是最具代表性的提升算法，提升树则是更具体的实例。

## AdaBoost算法

### 提升方法的基本思路

Kearns和Valient:
>**强可学习**: 在概率近似正确学习(PAC)的框架下，一个概念（类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是抢可学习的。

>**弱可学习**:一个概念，如果存在一个多项式的学习算法可以学习它，学习的正确率仅比随机猜测略好,那么就称这个概念是弱可学习的。

>在概率近似正确学习框架下, 强可学习与弱可学习是等价的。

根据这一理论，我们自然而然的想到是否可以通过容易发现的“弱学习算法”来提升为“强学习算法”。

这类想法中最具代表性的就是AdaBoost algorithm，他通过改变样本的权值，反复学习得到弱分类器，然后这些弱分类器按照一定的权值组合成一个强分类器。

具体的想法如下:
>1、提高前一轮弱分类器错误分类样本的去呀不照顾

>2、加大分类误差率小的弱分类器的权值，减小分类误差率小的弱分类器权值

### AdaBoost 算法

假设给定一个二类分类的训练数据集
$$T=\{(x_{1}, y_{1}), (x_{2}, y_{2}), \cdots, (x_{N}, y_{N})\}$$
其中，每个样本点由实例与标记组成。 实例$x_{i} \in \mathcal{X} \subseteq \boldsymbol{R}^n$, 类标记$y_{i} \in \mathcal{Y}=\{ -1, +1\}$。

** AdaBoost 算法 **

>输入: 训练数据集T; 弱学习算法

>输出: 最终分类器$G(x)$

>(1) 初始化训练的权值分布
>$$D_{1} = (w_{11}, \cdots, w_{1i}, \cdots, w_{1N}), w_{1i} = \frac{1}{N}$$

>(2) 对m = 1, 2, ..., M

>(a)使用权值分布$D_{m}$的训练数据集学习, 得到基本分类器
>$$G_{m}(x): \mathcal{x} \to {-1, +1}$$

>(b)计算$G_{m}$在训练数据集上的分类误差率
>$$e_{m} = P(G_{m}(x) \ne y_{i}) = \sum\limits_{i=1}^{N}{w_{mi}I(G_m(x_{i}) \ne y_{i})}$$

>(c)更新$G_{m}$的系数
>$$\alpha_{m} = \frac{1}{2} \ln{\frac{1-e_{m}}{e_{m}}}$$

>(d)更新训练数据集的权值分布
>$$D_{m+1} = ( \cdots, w_{m+1, i}, \cdots )$$

>$$w_{m+1, i} = \frac{w_mi}{Z_{m}}e^{-\alpha _{m}y_{i}G_{m}(x_{i})}$$

>其中, $Z_{m}$是规范化因子,它可以是$D_{m}$成为一个概率分布
>$$Z_{m} = \sum\limits_{i=1}^{N}{w_{mi}e^{-\alpha _{m}y_{i}G_{m}(x_{i})}}$$

>(3) 构建基本分类器的线性组合
>$$f(x) = \sum\limits_{m=1}^{M}{\alpha_{m}G_{m}(x)}$$

>并得到最终分类器
>$$G(x) = sign(f(x))$$

在上述算法中,分类误差率
$$e_{m} = \sum\limits_{G(x_{i} \ne y_{i})}{w_{mi}},$$
也就是分类误差率等于被分类器$G_{m}$误分类样本的权值之和.

当$e_{m} \leq \frac{1}{2}$时, $\alpha_{m} \geq 0$且$\alpha_{m}$的值随着$e_{m}$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。

$D_{m+1}$其实等价于
$$w_{m+1, i} =
\begin{cases}
\frac{w_{mi}}{Z_{m}}e^{-\alpha_{m}},&  &G_{m}(x_{i}) \ne y_{i}\\
\frac{w_{mi}}{Z_{m}}e^{\alpha_{m}},&  &G_{m}(x_{i}) \eq y_{1}
\end{cases}$$

很容易就可以看到, 被$G_{m}$误分类的样本权重得以扩张而正确分类的样本权重却得以缩小。(误分类样本权重/原分类样本权值)/(正确分类样本权重/原分类样本权重)$e^{2\alpha_{m}}=\frac{e_{m}}{1-e_{m}}$. 因此, 误分类率越大,错误样本权重被放大的程度也就越大。

线性组合$f(x)$表示最终分类器是M个基本分类器的加权表决, f(x)的绝对值表示确信度，f(x)的符号表示类的标记。

### AdaBoost 的python实现

[AdaBoost algorithm的python实现](https://github.com/forin-xyz/statistical-learning-method/blob/master/AdaBoost.py "gihub")


    import numpy as np
    from functools import reduce
    
    def sign(x):
        if x >= 0:
            return 1
        else:
            return -1
    
    def error_rate(X, y, gm, d=None):
        N = X.shape[0]
        if d is  None:
            d = np.ones(N)/N
        e = 0
        for i in range(N):
            if gm(X[i]) != y[i]:
                e += d[i]
        return e
    
    def AdaBoost_train(X, y, g):
        """
        X,y 训练数据集
    
        g
    
        callable, 接收三个变量，
        分别是训练数据X,y 以及权值训练数据的权值d
        输出也是一个callable对象,表示得到的基本分类器
    
        Return:
        基本分类器、基本分类器的权重以及
        最终的分类器f, 是一个callable对象
        """
        X = np.array(X)
        y = np.array(y)
        alpha_list = []
        glist = []
        N = X.shape[0]
        d = np.ones(N)/N
        gm = g(X, y, d)
        e = error_rate(X, y, gm, d)
        alpha = 0.5*np.log(1/e - 1)
        alpha_list.append(alpha)
        glist.append(gm)
        f = lambda x: sign(reduce(lambda s, t: s+t, [a*g(x) for a, g in tuple(zip(alpha_list, glist))]))
        lll = 0
        while any( (f(X[i]) != y[i] for i in range(N)) ) and lll < 3:
            lll += 1
            for i in range(N):
                if gm(X[i]) != y[i]:
                    d[i] = d[i] * np.e ** alpha
                else:
                    d[i] = d[i] * np.e ** (-alpha)
            zm = d.sum()
            d = d / zm
            gm = g(X, y, d)
            e = 0
            for i in range(N):
                if gm(X[i]) != y[i]:
                    e += d[i]
            alpha = 0.5 * np.log(1/e - 1)
            alpha_list.append(alpha)
            glist.append(gm)
            f = lambda x: sign(reduce(lambda s, t: s+t, [a*g(x) for a, g in tuple(zip(alpha_list, glist))]))
        return alpha_list, glist, f
    
    def test_classify(X, y, d):
        vs = np.arange(-1, 10) + 0.5
        g1 = lambda x, v: sign(x - v)
        g2 = lambda x, v: sign(v - x)
        e = 1
        g = None
        for v in vs:
            gv1 = lambda x, v=v: g1(x, v)
            gv2 = lambda x, v=v: g2(x, v)
            e1 = error_rate(X, y, gv1, d)
            e2 = error_rate(X, y, gv2, d)
            if e1 < e:
                e = e1
                g = gv1
            if e2 < e:
                e = e2
                g = gv2
        return g
    
    def test_AdaBoost_train():
        X = [
            [0], [1], [2], [3], [4], [5], [6], [7], [8], [9]
        ]
        y = [
            1, 1, 1, -1, -1, -1, 1, 1, 1, -1
        ]
        g = test_classify
        alpha_list, glist, f = AdaBoost_train(X, y, g)
        print(alpha_list)
        if ((np.array(alpha_list) - np.array([0.4236, 0.6496, 0.7541]))**2).sum() < 1e5:
            print('test AdaBoost_train successfully!')
        else:
            print('test AdaBoost_train failed!')
    
    if __name__ == '__main__':
        test_AdaBoost_train()

### AdaBoost 训练误差分析

**定理1(AdaBoost的误差界)** AdaBoost算法的最终分类器的训练误差界为
$$
\frac{1}{N}\sum\limits_{i=1}^{N}I(G(x_{i} \ne y_{i})) \leq \frac{1}{N}\sum\limits_{i=1}{N}e^(-y_{i}f(x_{i})) = \prod\limits_{m}{Z_{m}}
$$

证明:
>当$G(x_{i}) \ne y_{i}$时, $y_{i}f(x_{i})=<0$, $I(G(x_{i}) \ne y_{i}) = 1 =< e^{-y_{i}f(x_{i})}$

>当$G(x_{i}) \eq y_{i}$时, $I(G(x_{i}) \ne y_{i}) = 0 < e^{-y_{i}f(x_{i})}$

> 于是左边的不等式得到了证明

>至于右边的等式, 有
>$$\begin{aligned}\frac{1}{N}\sum\limits_{i}exp(-y_{i}f(x_{i}))&\\
>=&\sum\limits_{i}w_{1i}\prod\limits_{m}{exp(-\alpha_{m}y_{i}G_{m}(x_{i}))}\\
>=&Z_{1}\sum\limits_{i}w_{2i}\prod\limits_{m=2}^{M}{exp(-\alpha_{m}y_{i}G_{m}(x_{i}))}
>=&Z_{1}Z_{2}\cdotsZ_{M}\\
>=&\prod\limits_{m}{Z_{m}}
>\end{aligned}$$

> 于是右边也得到了证明

这一定理说明，可以在每一轮选取适当的$G_{m}$使得$Z_{m}$最小，从而使得训练误差下降最快。

**定理2(二类分类问题AdaBoost的训练误差界)**
$$
\prod\limits_{m=1}^{M}{Z_{m}} = \prod\limits_{m=1}^{M}{2\sqrt{e_{m}(1-e_{m})}} = \prod\limits_{m=1}^{M}{\sqrt{(1-4*\gamma_{m}^2)}} = \leq exp(-2\sum\limits_{m=1}^{M}(\gamma_{m}))
$$
这里$\gamma_{m} = \frac{1}{2} - e_{m}$.

证明提示:
>可以由$e^x$和$\sqrt{1-x}$在$x=0$附近的泰勒展开式推出不等式$\sqrt{(1-4\gamma_{m})^2} \leq exp(-2\gamma_{m}^2)$

Ada 是 adaptive的简写，是适应的意思

**推论(训练误差指数下降)**
如果存在$\gamma \ge 0$, 对所有的m都有$\gamma_{m}
\geq \gamma$,则
$$
\frac{1}{N}\sum_{i=1}{N}{I(G(x_{i}) \ne y_{i})} \leq exp(-2M\gamma ^{2})
$$

## AdaBoost 算法的解释

AdaBoost算法还可以解释为模型为加法模型，损失函数为指数函数，学习算法为前向分步算法式的二类分类算法。

加法模型:
$$
f(x)=\sum\limits_{m=1}{M}\beta_{m}b(x;\gamma_{m})
$$

学习加法模型等价与经验风险最小化的优化问题
$$
\min\limits_{\beta_{m},\gamma_{m}}{\sum\limits_{i=1}^{N}L(y_{i}, f(x_{i}))}
$$

## 提升树


## 中英文对照

提升 boosting

提升树 boosting tree

强可学习 strong learnable

弱可学习 weakly learnable

概率近似正确 probably approximately correct, PAC

加法模型 additive model

假设空间 hypothesis space

前向分步算法 forward stagewise algorithm

决策树桩 decision stump
