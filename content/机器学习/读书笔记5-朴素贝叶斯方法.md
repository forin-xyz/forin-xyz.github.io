Title: 《统计学习方法》读书笔记5-朴素贝叶斯方法，含python源码
Date: 2016-02-21 23:00
Category: 机器学习
Tags: 机器学习, 朴素贝叶斯, Naive Bayes, python, 计算机科学, 数学原理, 读书笔记
Summary: 朴素贝叶斯方法(Naive Bayesian Method)是基于贝叶斯定理与特征 条件独立假设的分类方法，是一种常见和高效的学习方法。

## 原理

### 贝叶斯定理

事件A和事件B同时发生的概率等于事件B发生的概率与事件A在事件B已经发生的条件下发生的概率的乘积, 即
$$ P(A, B) = P(AB) = P(A|B)P(B) $$

### 贝叶斯公式

$$ P(A_{i}|B) = \frac{P(B|A_{i})P(A_{i})}{P(B)} $$
其中
$$ P(B) = \sum\limits_{i=0}^{n}P(B|A_{i})P(A_{i}) $$
于是
$$ P(A_{i}|B) = \frac{P(B|A_{i})P(A_{i})}{\sum\limits_{i=0}^{n}P(B|A_{i})P(A_{i})} $$
这就是贝叶斯公式, 其中$\{A_{i}\}$为完备事件组,即$\bigcup \limits_{i}A_{i} = \Omega, A_{i}A_{j} = \phi,(i \ne j), P(A_{i}) > 0$.

### 朴素贝叶斯方法的基本描述

输入空间$\mathcal{X} \subseteq \mathrm{R}^{n}$是n维向量的几何
输出空间$\mathcal{Y} = \{c_{1}, c_{2}, \cdots, c_{K}\}$是类标记组合
朴素贝叶斯方法是求特定输入下的后验概率最大化。

### 特征条件相互独立假设

朴素贝叶斯方法对条件概率分布做了互相独立的假设，即在输出一定的条件下，实例点的特征分量彼此相互独立，即：
$$\begin{aligned}
P(X = x|Y = c_{k}) =& P(X^{1}=x^{1}, \cdots, X^{n}=x^{n}|Y=c_{k})\\
=&\prod\limits_{j=1}^{n}P(X^{j}=x^{j}|Y=c_{k})
\end{aligned}
$$

### 朴素贝叶斯方法的最终推导
将特征条件相互独立代入到贝叶斯公式， 有
$$
P(Y=c_{k}|X=x)=\frac{P(Y=c_{k})\prod\limits_{j=1}^{n}P(X^{j}=x^{j}|Y=c_{k})}{\sum_{k}{P(Y=c_k)}\prod_{j}{P(X^{j}=x^{j}|Y=c_{k})}}
$$
朴素贝叶斯分类器就是选取在给定的输入的情况下，将后验概率最大的类最为输出的分类的方法，又
由于分子对于每一个给定的$c_{k}$是相同的,于是
$$
y = \arg\max\limits_{c_{k}}{P(Y=c_{k})}\prod\limits_{j=1}^{n}{P(X^{j}=x^{j}|Y=c_{k})}
$$
就是朴素贝叶斯分类器的最终模式。

### 后验概率最大化的意义

朴素贝叶斯方法将实例分到后验概率最大的类中，等价于0-1损失函数下期望风险最小化。

假设联合概率分布为$P(X,Y)$, 对于给定的决策函数$f$，期望风险
$$R_{exp}(f) = E_{X}\sum\limits_{k=1}{K}\lbrack L(c_{k}, f(X))\rbrack P(c_{k}|X)
$$
要使得期望风险最小，于是
$$\begin{aligned}
f(x) =& \arg \min\limits_{c_{k}}P(y \neq c_{k}|X=x)
=& \arg \min\limits_{c_{k}}{(1-P(y=c_{k}|X=x))} \\
=& \arg \max\limits_{c_{k}}P(c_{k}|X=x)
\end{aligned}$$


## 朴素贝叶斯方法的参数估计

### 极大似然估计

$$P(Y=c_{k}) = \frac{freq(y=c_{k})}{N}$$
其中$freq$为频数函数。
$$P(X^{(j)}=a_{jl}|c_{k})=\frac{freq(x^{(j)}=a_{jl},c_{k})}{freq(c_{k})}$$
其中$a_{jl}$表示第j个特征可能取得的第l个值。
将上述两式带入到朴素贝叶斯分类器模型就可以得到极大似然估计。

#### 朴素贝叶斯方法的极大似然估计算法

*naive Bayes algorithm* **朴素贝叶斯算法**

>输入: 训练数据集$T=\{(x_{1},y_{1}), \cdots, (x_{N}, y_{N})\}$,其中$x_{i}=(x_{i}^{(1)}, \cdots, x_{i}^{(n)})^{T}$, $x_{i}^{(j)}$是第i个样本的第j个特征, $x_{i}^{(j)} \in {a_{j1}, \cdots, a_{jS_{j}}}$, $a_{jh}是第j个特征可能取得的第l个值, $y_{i} \in \{c_{1}, \cdots, c_{K}\}$; 实例$x$.

>输出: 实例x的分类

>(1) 计算先验概率和条件概率

>$$P(c_{k})$$

>$$P(a_{jh}|c_{k})$$

>(2) 对于给定的实例, 计算

>$$P(c_{k})\prod\limits_{j=1}^{n}P(x^{(j)}|c_{k})$$

>(3) 取上面结果最大的类作为实例$x$的类输出。


#### 朴素贝叶斯方法的极大似然估计的python实现

[朴素贝叶斯-极大似然估计](https://github.com/forin-xyz/statistical-learning-method/blob/master/naive_bayes_MLE.py "github")

    import numpy as np

    def naive_bayes_MLE_train(x, X, y, xl=None, yl=None):
        x = np.array(x)
        X = np.array(X)
        y = np.array(y)
        N, n = X.shape
        if not xl:
            xl = [list(set(X[:,i])) for i in range(n)]
        if not yl:
            yl = list(set(y))
        K = len(yl)
        # 统计y各个结果出现的频次
        yf = np.array([(y==ylit).sum() for ylit in yl])
        # 统计x^j,y出现的频次
        xfmatrix = np.zeros((K, n, np.max([len(xlit) for xlit in xl])))
        for i in range(N):
            k = yl.index(y[i])
            for j in range(n):
                h = xl[j].index(X[i][j])
                xfmatrix[k,j,h] += 1
        # 计算出各个类后验概率的分母
        py = yf/N
        pxmatrix = np.array([xfmatrix[k] / yf[k] for k in range(K)])
        pyx = np.zeros(K)
        for k in range(K):
            pkx = py[k]
            for j in range(n):
                pkx *= pxmatrix[k, j, xl[j].index(x[j])]
            pyx[k] = pkx
        return yl[np.argmax(pyx)]

    # 测试

    def test_naive_bayes_MLE_train():
        X = [[1, 'S'], [1, 'M'], [1, 'M'], [1, 'S'], [1, 'S'],
             [2, 'S'], [2, 'M'], [2, 'M'], [2, 'L'], [2, 'L'],
             [3, 'L'], [3, 'M'], [3, 'M'], [3, 'L'], [3, 'L']
            ]
        y = [-1, -1, 1, 1, -1, -1, -1,
             1, 1, 1, 1, 1, 1, 1, -1]
        xi = [2, 'S']
        yo = naive_bayes_MLE_train(xi, X, y)
        if yo == -1:
            print('test naive_bayes_MLE_train successed!')
        else:
            print('test naive_bayes_MLE_train failed!')


    if __name__ == '__main__':
        test_naive_bayes_MLE_train()

### 贝叶斯估计

贝叶斯估计等价于在随机变量各个取值的频数上赋予一个正数$\lambda$.

$$P_{\lambda}(X^{(j)}=a_{jl}|c_{k})=\frac{freq(x^{(j)}=a_{jl},c_{k})+\lambda}{freq(c_{k})+S_{j}\lambda}$$
其中$S_{j}$表示第j个特征可能取值的集合的元素个数。
$$P_{\lambda}(Y=c_{k}) = \frac{freq(y=c_{k})+\lambda}{N+K\lambda}$$

#### 朴素贝叶斯方法的贝叶斯估计的python实现


[朴素贝叶斯-贝叶斯估计](https://github.com/forin-xyz/statistical-learning-method/blob/master/naive_bayes_BE.py "github")


    """
    贝叶斯方法的贝叶斯估计实现
    """
    
    import numpy as np
    
    def naive_bayes_BE_train(x, X, y, xl=None, yl=None, lambdas=0):
        x = np.array(x)
        X = np.array(X)
        y = np.array(y)
        N, n = X.shape
        if not xl:
            xl = [list(set(X[:,i])) for i in range(n)]
        if not yl:
            yl = list(set(y))
        K = len(yl)
        # 统计y各个结果出现的频次
        yf = np.array([(y==ylit).sum() for ylit in yl])
        # 统计x^j,y出现的频次
        xfmatrix = np.zeros((K, n, np.max([len(xlit) for xlit in xl])))
        for i in range(N):
            k = yl.index(y[i])
            for j in range(n):
                h = xl[j].index(X[i][j])
                xfmatrix[k,j,h] += 1
        # 计算出各个类后验概率的分母
        py = (yf + lambdas)/(N+K*lambdas)
        pxmatrix = np.array([[(xfmatrix[k, j, :]+lambdas) / (yf[k] + len(xl[j]) * lambdas) for j in range(n)] for k in range(K)])
        pyx = np.zeros(K)
        for k in range(K):
            pkx = py[k]
            for j in range(n):
                pkx *= pxmatrix[k, j, xl[j].index(x[j])]
            pyx[k] = pkx
        return yl[np.argmax(pyx)]
    
    
    # 测试
    
    def test_naive_bayes_BE_train():
        X = [[1, 'S'], [1, 'M'], [1, 'M'], [1, 'S'], [1, 'S'],
             [2, 'S'], [2, 'M'], [2, 'M'], [2, 'L'], [2, 'L'],
             [3, 'L'], [3, 'M'], [3, 'M'], [3, 'L'], [3, 'L']
            ]
        y = [-1, -1, 1, 1, -1, -1, -1,
             1, 1, 1, 1, 1, 1, 1, -1]
        xi = [2, 'S']
        yo = naive_bayes_BE_train(xi, X, y, lambdas=1)
        if yo == -1:
            print('test naive_bayes_BE_train successed!')
        else:
            print('test naive_bayes_BE_train failed!')
    
    
    if __name__ == '__main__':
        test_naive_bayes_BE_train()


## 朴素贝叶斯方法的应用

### 垃圾邮件过滤器

>一个特定的单词组{d1, d2, d3, ..., dn}, 一个可用的邮件库, 统计每个邮件中对应单词出现与否以及是否是垃圾邮件,这样就得到一个训练集

>对于未知的一份邮件, 就可以采用朴素贝叶斯方法对邮件是否是垃圾邮件进行判断

>这里假设在给定的分类下每个单词出现与否与其他单词无关，虽然这和实际情况有点儿出入，但是它的预测效果依然不错。


### 特征条件独立分布的影响

这一假设使得朴素贝叶斯方法变得简单，但是在有些时候会牺牲一些分类正确性。

### 更多

如果假设特征的各个维度之间存在一定的概率依存关系，模型就编程了贝叶斯网络。


## 中英文对照

朴素贝叶斯方法 Naive Bayesian Method

类标记 class label

贝叶斯定理 bayes theorem

贝叶斯公式 Bayes Rule

先验概率 Prior Probability

后验概率 Posterior Probability

极大似然估计 Maximum Likelihood Estimation, ELM

贝叶斯估计 Bayesian Estimation

拉普拉斯平滑 Laplace smoothing


