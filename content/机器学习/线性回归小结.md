Title: 机器学习之线性模型
Date: 2016-10-12 23:08
Category: 机器学习
Tags: 线性模型
Summary: 线性模型的机器学习的基础。

## Abastract
线性模型，故名司仪，就是预测值可以表达成特征的线性组合的模型。

即
$$
\widehat{y} = \mathbf{w}^{T} x + b
$$

## 最小二乘法

$$
loss = \sum_{i=0}^{N}(\hat{y} - y)^2
$$
最小化loss可以得到唯一解,这就叫做最小二乘法。
相当于认为模型的预测值和实际值的残差满足正态分布。
$$
P(m|D) = \frac{P(m, D)}{P(D)} \varpropto P(m, D) = P(D|m)P(m)
$$
如果模型m是均匀分布,模型产生的数据残差满足正态分布的化,对上式进行求对数,取最大值就等价于损失函数求最小值。

## 岭回归　Ridge Regression
如果模型不是均匀分布的,那么我们就要在损失函数上增加一个惩罚项,代表贝叶斯公式里面的P(m),这也就是奥卡姆剃刀的原理，也就是越简单的模型他的先验概率越大。

$$
loss = \sum_{i=0}^{N}(\hat{y} - y)^2 + \lambda{\lVert \mathbf{w} \rVert}^2
$$
最小二乘法是一种无偏估计,它容易产生过拟合,且在样本各数小于特征各数时,最小二乘法得不到有意义的结果,这时候就是岭回归发挥作用的时候了。

## 分位数回归 Quantile Regression

最小二乘法关注的是平均值,假定的是残差满足正态分布,分位数回归关注的是分位数,而且可以求出一系列的曲线,通过设定不同的分位数来观察整个数据的分布情况，它也就不要正态同分布的假设了。

$$
loss = \sum_{i=0}^N{\rho_{\tau}
}{(\widehat{y} - y)}
$$
其中
$$
\rho_{\tau}(u)　＝　u(I(u \gt 0) - \tau)
$$
$$
0 \lt \tau \lt 1
$$


## 泊松回归　Possion Regression
泊松回归是对响应变量是整数的回归模型,例如网页的点击次数,一个月下雨的天数,德国队和巴西队比赛结果会是几比几等。

$$
loss = \sum{(\widehat{y}_i　+ \log(y_i!) - y_i\log{\widehat{y}_i})}
$$

泊松分布
$$
Possion(y|\lambda) = \frac{{\lambda}^y{e^{-\lambda}}}{y!}
$$
假设
$${\lambda} = \mathbf{w}^T x +b $$
或者
$$
\log{\lambda} = \mathbf{w}^T x +b
$$
最后使用均值估计Y的预测值,也就是
$$
\widehat{y} = var(\lambda) = \lambda
$$
然后使用MLE即可以得到上述的损失函数。

泊松部分可以用来计算用户的CTR也就是
$$
CTR = \frac{\lambda_{click}+\alpha}{\lambda_{view}+\beta}
$$
其中alpha, beta表示先验统计,用于平滑曲线的。

## Logistic Regression
逻辑回归也是一种广义线性模型,使用于分类等判断是或者否的问题。
$$
P(y=1|\mathbf{w},b) = \frac{1}{1+e^{-\mathbf{w}^Tx - b}} 
$$
然后使用交叉熵或者极大似然估计可以得到他的函数
$$
loss = -\frac{1}{N}\sum_{i=0}^N(y\log{(\widehat{y}_i)} - (1-y_i)\log{(1-\widehat{y}_i)})
$$

## Softmax Regression
Softmax Regression属于逻辑回归的多类推广
$$
P(y|m) =  \frac{e^{\mathbf{W}^Tx} + b}{Z}
$$
其中Z是规范化因子,保证概率和为1.

## 线性回归用在哪

深度学习模型比较适用于确定性事件,即一只猫的照片不可能变成一个人的照片,只要人类去标记,那就由0.99以上的可能可以标记出来。
决策树是一个很好的模型,但是如果数据中存在极多的噪音的时候,决策树就会陷入无厘头的左右摇摆上。
而线性模型就非常适合与上述的各种情况,可以对一个不可测的事件进行一定程度上的预测，又可以很好的适用于大量噪音存在的情况。

而且深度学习的激活函数现在也普遍使用relu和maxout这样的激活函数取代过去的sigmoid,tanh这样的非线性函数,最终的输出层,深度学习也多考虑使用sigmoid和softmax这样的广义线性模型。
可以说线性模型是机器学习的基础，特别适合于不确定性比较强的场合。
